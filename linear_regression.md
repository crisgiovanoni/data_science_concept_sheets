\begin{table}[]
\begin{tabular}{|l|l|l|l|l|}
\hline
Category                        & Method                                                                                                       & Description                                                                                                                                                                                                                                                                                                & Input                                                                                                                                                                                                           & Return                                                                                                                                                                                       \\ \hline
Filter Methods                  & Correlation Thresholds                                                                                       & -Use correlation matrix and decide what new features to create                                                                                                                                                                                                                                             & \begin{tabular}[c]{@{}l@{}}1. data frame\\ 2. .corr() (pandas)\\ 3. sns.heatmap\end{tabular}                                                                                                                    & 1. Heatmap with annotated R                                                                                                                                                                  \\ \hline
                                & SelectKBest                                                                                                  & \begin{tabular}[c]{@{}l@{}}-Removes all but highest scoring features\\ -Looks for:\\ a. Score as test statistic\\ b. P-value\\ -F-regression or Chi-square\\ -T-tests tells if a single variable is statistically significant; F-test tells if a group of\\ variables are jointly significant\end{tabular} & \begin{tabular}[c]{@{}l@{}}1. SelectKBest method\\ 2. f\_regression method (sklearn.feature\_selection)\end{tabular}                                                                                            & 1. Selected features that are significant                                                                                                                                                    \\ \hline
Wrapper Methods                 & Backward Elimination                                                                                         & \begin{tabular}[c]{@{}l@{}}-Model is built already\\ -Checks performance of model then iteratively remove the worst performing features one by one until\\ performance is "acceptable"\\ -Performance metric is p-value. p-value \textgreater 0.05, remove feature\end{tabular}                            & \begin{tabular}[c]{@{}l@{}}1. LRmodel (sm.OLS)\\ 2. X and y\\ 3. Conditional (while loop or,boolean mask) that generates p-value and checks for and removes feature with p-value \textgreater 0.05\end{tabular} & 1. Resulting feature/s from condition (usually, column name)                                                                                                                                 \\ \hline
                                & Recursive Feature Elimination (RFE)                                                                          & \begin{tabular}[c]{@{}l@{}}-Recursively removes attributes then builds model on the remaining attributes\\ -Yields accuracy metric to rank feature according to importance\end{tabular}                                                                                                                    & \begin{tabular}[c]{@{}l@{}}1. LReg model\\ 2. RFE method\\ 3. X and y\end{tabular}                                                                                                                              & \begin{tabular}[c]{@{}l@{}}1. rfe.ranking\_ (ranking of variables based on importance level, 1=important)\\ 2. rfe.support\_(support of variables (True/False, keep or discard)\end{tabular} \\ \hline
                                & Forward Selection                                                                                            &                                                                                                                                                                                                                                                                                                            &                                                                                                                                                                                                                 &                                                                                                                                                                                              \\ \hline
Embedded Methods                & LassoCV                                                                                                      & \begin{tabular}[c]{@{}l@{}}-Penalizes irrelevant features by making its coefficient=0.\\ -Weights the coefficient by virtue of relevance\\ -High α ≈ More non-zero value features\end{tabular}                                                                                                             & \begin{tabular}[c]{@{}l@{}}1. LReg model\\ 2. LassoCV method (sklearn.linear\_model)\end{tabular}                                                                                                               & \begin{tabular}[c]{@{}l@{}}1. alpha\_\\ 2. coef\_\end{tabular}                                                                                                                               \\ \hline
                                & \begin{tabular}[c]{@{}l@{}}Others:\\ -Elastic Net\\ -Ridge Regression\\ -Regularized Regression\end{tabular} &                                                                                                                                                                                                                                                                                                            &                                                                                                                                                                                                                 &                                                                                                                                                                                              \\ \hline
Linear Dimensionality Reduction & Principal Component Analysis (PCA)                                                                           & \begin{tabular}[c]{@{}l@{}}-Creates a new feature, aka Principal Component which combines X1 and X2\\ -Ranked in order of explained variance\\ -Only takes in normalized dataset\\ -New Principal Components are not interpretable\end{tabular}                                                            & 1. PCA method (sklearn.decomposition)                                                                                                                                                                           & \begin{tabular}[c]{@{}l@{}}1. n\_components\_\\ 2. pca\_explained\_variance\_ratio\_\end{tabular}                                                                                            \\ \hline
\end{tabular}
\end{table}
